# -*- coding: utf-8 -*-
"""
RL_model.py â€” Q-learning ê¸°ë°˜ìœ¼ë¡œ Validation ë°°ì¹˜ë§ˆë‹¤ 'íŒŒì¸íŠœë‹ ì‹¤í–‰ ì—¬ë¶€/ë²”ìœ„'ë¥¼ ê²°ì •í•´
ëª¨ë¸ì„ ë¯¸ì„¸ì¡°ì •í•˜ëŠ” ëª¨ë“ˆ.

êµ¬ì„±:
- ìƒíƒœ(STATES): ['NI_1','1_2','2_3','3_PF']  (train lossì˜ Î¼, Ïƒë¡œ ì •ì˜í•œ êµ¬ê°„)
- í–‰ë™(ACTIONS): ['FT_NONE','FT_DEC','FT_ALL']  (ë¬´FT / decoderë§Œ FT / ì „ë¶€ FT)
- ë³´ìƒ(REWARD):  Î”batch loss = Loss_before - Loss_after  (í•´ë‹¹ ë°°ì¹˜ì˜ ì¬êµ¬ì„± MSE ê°ì†ŒëŸ‰)

ì™¸ë¶€ ì˜ì¡´:
- config.py: CHK_PATH, PICKLE_PATH, í•˜ì´í¼íŒŒë¼ë¯¸í„°(SEQ_LEN ë“±)
- model(MTSTAutoencoder)
- load_tr_loss_stat(): PICKLE_PATH['tr_loss_stat']ì—ì„œ Î¼, Ïƒë¥¼ ë°”ë¡œ ë¡œë“œ (ì˜ˆì™¸ ì²˜ë¦¬ ì—†ìŒ)
"""

###########################################################################################################
# import libraries
###########################################################################################################

import pandas as pd
import numpy as np
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader, TensorDataset
import matplotlib.pyplot as plt
from config import *
import pickle
import copy, random, numpy as np, torch
from typing import Dict, List, Tuple, Optional
from typing import Dict, List, Tuple, Optional
from TAD_model import MTSTAutoencoder
import torch.nn.functional as F

###########################################################################################################
# set user defined functions
###########################################################################################################

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

def set_seed(seed: Optional[int] = None):
    """
    ì¬í˜„ì„±ì„ ìœ„í•´ íŒŒì´ì¬/ë„˜íŒŒì´/íŒŒì´í† ì¹˜ ì‹œë“œë¥¼ ê³ ì •í•œë‹¤.

    Args:
        seed (int|None): ê³ ì •í•  ì‹œë“œ. Noneì´ë©´ ì•„ë¬´ ì‘ì—…ë„ í•˜ì§€ ì•ŠìŒ.
    """
    if seed is None:
        return
    random.seed(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)

    if torch.cuda.is_available():
        torch.cuda.manual_seed_all(seed)
        # ì™„ì „í•œ ì¬í˜„ì„±ì„ ìœ„í•´ ì•„ë˜ ë‘ ì˜µì…˜ì„ ë¹„ê²°ì •ì ìœ¼ë¡œ ë§Œë“œëŠ” ìµœì í™”ë¥¼ ë”
        torch.backends.cudnn.deterministic = True
        torch.backends.cudnn.benchmark = False


def set_ft_by_action(model: nn.Module, action: str):
    """
    í–‰ë™(action)ì— ë”°ë¼ ì–´ë–¤ íŒŒë¼ë¯¸í„°ë¥¼ í•™ìŠµí• ì§€ ìŠ¤ìœ„ì¹­í•œë‹¤.

    ACTIONS:
        - 'FT_NONE': ëª¨ë“  íŒŒë¼ë¯¸í„° ë™ê²° (í‰ê°€ë§Œ)
        - 'FT_ENC': encoderë§Œ í•™ìŠµ
        - 'FT_DEC' : decoderë§Œ í•™ìŠµ
        - 'FT_ALL' : ëª¨ë“  íŒŒë¼ë¯¸í„° í•™ìŠµ

    Args:
        model (nn.Module): MTSTAutoencoder ì¸ìŠ¤í„´ìŠ¤
        action (str): ìœ„ ACTIONS ì¤‘ í•˜ë‚˜
    """
    # freeze all parameters
    for p in model.parameters():
        p.requires_grad = False

    if action == 'FT_NONE':
        pass
    elif action == 'FT_ENC': # fine-tune encoder (self.layers)
        for lyr in model.layers:
            for p in lyr.parameters():
                p.requires_grad = True
    elif action == 'FT_DEC': # fine-tune decoder parameters
        for p in model.decoder.parameters():
            p.requires_grad = True
    elif action == 'FT_ALL': # fine-tune all parameters
        for p in model.parameters():
            p.requires_grad = True
    # end if


def make_optimizer(model: nn.Module, lr: float = 5e-4, wd: float = 0.0):
    """
    í˜„ì¬ requires_grad=Trueì¸ íŒŒë¼ë¯¸í„°ë§Œ ëª¨ì•„ Adam ì˜µí‹°ë§ˆì´ì €ë¥¼ ë§Œë“ ë‹¤.

    Args:
        model (nn.Module): í•™ìŠµ ëŒ€ìƒ ëª¨ë¸
        lr (float): í•™ìŠµë¥ 
        wd (float): weight decay

    Returns:
        torch.optim.Optimizer | None: í•™ìŠµ íŒŒë¼ë¯¸í„°ê°€ ì—†ë‹¤ë©´ None ë°˜í™˜
    """
    params = [p for p in model.parameters() if p.requires_grad]
    return torch.optim.Adam(params, lr=lr, weight_decay=wd) if params else None


@torch.no_grad()
def batch_loss_eval(model: nn.Module, xb: torch.Tensor, base_dim: int = BASE_DIM) -> float:
    """
    ë°°ì¹˜ í…ì„œ xbì— ëŒ€í•´ ì¬êµ¬ì„± MSE ì†ì‹¤ì„ ê³„ì‚°(í‰ê°€ ëª¨ë“œ)í•œë‹¤.

    Args:
        model (nn.Module): í‰ê°€í•  ëª¨ë¸
        xb (Tensor): shape (B, L, D) ë°°ì¹˜ ì…ë ¥
        base_dim (int): ì†ì‹¤ ê³„ì‚°ì— ì‚¬ìš©í•  ì…ë ¥ í”¼ì²˜ ì°¨ì› ìˆ˜ (ì•ìª½ BASE_DIMë§Œ ì‚¬ìš©)

    Returns:
        float: ë°°ì¹˜ í‰ê·  ì¬êµ¬ì„± MSE
    """
    model.eval()
    rec = model(xb)
    
    # loss ê³„ì‚°: ì „ì²´ ìš”ì†Œ í‰ê·  (B,L,D ëª¨ë‘ í‰ê· )
    loss = F.mse_loss(rec[:, :, :base_dim], xb[:,  :, :base_dim], reduction='mean')

    return float(loss.item())


def load_tr_loss_stat(path: str) -> Tuple[float, float]:
    """
    í”¼í´ íŒŒì¼ì—ì„œ í•™ìŠµ ì†ì‹¤ í†µê³„ (mu, std)ë¥¼ 'ì§ì ‘' ë¡œë“œí•œë‹¤. ì˜ˆì™¸ ì²˜ë¦¬ ì—†ìŒ.

    í—ˆìš© í¬ë§·:
        - {'mean': <float>, 'std': <float>}
        - {'loss': {'mean': <float>, 'std': <float>}}

    Args:
        path (str): PICKLE_PATH['tr_loss_stat']

    Returns:
        (mu, sigma) (float, float): í‰ê· ê³¼ í‘œì¤€í¸ì°¨

    Raises:
        KeyError: í—ˆìš© í¬ë§·ì´ ì•„ë‹Œ ê²½ìš°
    """
    with open(path, 'rb') as f:
        d = pickle.load(f)

    def to_float(x):  # numpy scalar ë“± ì•ˆì „ ë³€í™˜
        return float(np.array(x).astype('float64'))

    if isinstance(d, dict) and 'mean' in d and 'std' in d:
        mu, sigma = to_float(d['mean']), to_float(d['std'])
    elif isinstance(d, dict) and 'loss' in d and isinstance(d['loss'], dict):
        mu, sigma = to_float(d['loss']['mean']), to_float(d['loss']['std'])
    else:
        raise KeyError("train loss pickle must contain {'mean','std'} or {'loss':{'mean','std'}}")
    # end if

    # í‘œì¤€í¸ì°¨ 0/ìŒìˆ˜ ë³´í˜¸
    if sigma <= 0:
        sigma = 1e-12
    return mu, sigma


def bin_state(batch_loss: float, mean: float, std: float) -> str:
    """
    ë°°ì¹˜ ì†ì‹¤ì„ í•™ìŠµ í†µê³„ Î¼, Ïƒ ê¸°ì¤€ êµ¬ê°„ìœ¼ë¡œ ë§¤í•‘í•´ ìƒíƒœ(state)ë¥¼ ë°˜í™˜í•œë‹¤.

    STATES:
        - 'NI_1' : x < Î¼+1Ïƒ
        - '1_2'  : Î¼+1Ïƒ â‰¤ x < Î¼+2Ïƒ
        - '2_3'  : Î¼+2Ïƒ â‰¤ x < Î¼+3Ïƒ
        - '3_PF' : x â‰¥ Î¼+3Ïƒ

    Args:
        batch_loss (float): í˜„ì¬ ë°°ì¹˜ì˜ ì†ì‹¤
        mean (float): í•™ìŠµ ì†ì‹¤ í‰ê·  Î¼
        std (float): í•™ìŠµ ì†ì‹¤ í‘œì¤€í¸ì°¨ Ïƒ

    Returns:
        str: ìƒíƒœ ë¼ë²¨ ì¤‘ í•˜ë‚˜
    """
    s = max(float(std), 1e-12)
    m = float(mean)
    
    # t0, t1, t2, t3 = m, m + s, m + 2*s, m + 3*s
    # x = float(batch_loss)
    # if x < t0:
    #     return 'NI_0'
    # if x < t1: 
    #     return '0_1'
    # elif x < t2: 
    #     return '1_2'
    # elif x < t3: 
    #     return '2_3'
    # else: 
    #     return '3_PI'

    t0, t1_5, t3 = m, m + 1.5*s, m + 3*s
    x = float(batch_loss)
    if x < t0:
        return 'NI_0'
    if x < t1_5: 
        return '0_1.5'
    elif x < t3: 
        return '1.5_3'
    else: 
        return '3_PI'

class QTable:
    """
    ë¬¸ìì—´ ìƒíƒœ/í–‰ë™ ê³µê°„ì— ëŒ€í•œ ê°„ë‹¨í•œ Q(s,a) í…Œì´ë¸” êµ¬í˜„.

    Attributes:
        states (List[str]): ìƒíƒœ ë¼ë²¨ ëª©ë¡
        actions(List[str]): í–‰ë™ ë¼ë²¨ ëª©ë¡
        Q (Dict[(str,str), float]): (state, action) â†’ Qê°’
    """
    def __init__(self, states: List[str], actions: List[str]):
        self.states  = list(states)
        self.actions = list(actions)
        self.Q: Dict[Tuple[str,str], float] = {(s,a): 0.0 for s in self.states for a in self.actions}

    def greedy_action(self, s: str) -> str:
        """
        ì£¼ì–´ì§„ ìƒíƒœ sì—ì„œ Qê°’ì´ ìµœëŒ€ì¸ í–‰ë™ì„ ë°˜í™˜í•œë‹¤(íƒìš• ì •ì±…).

        Args:
            s (str): ìƒíƒœ ë¼ë²¨

        Returns:
            str: í–‰ë™ ë¼ë²¨
        """
        return max(self.actions, key=lambda a: self.Q[(s,a)])

    def update(self, s: str, a: str, r: float, s_next: str, alpha: float = 0.2, gamma: float = 0.9):
        """
        Q-learning ì—…ë°ì´íŠ¸ ì‹ìœ¼ë¡œ Q(s,a)ë¥¼ ê°±ì‹ í•œë‹¤.

        Q(s,a) â† (1-Î±)Q(s,a) + Î±[r + Î³ max_a' Q(s',a')]

        Args:
            s (str): í˜„ì¬ ìƒíƒœ
            a (str): ì‹¤í–‰í•œ í–‰ë™
            r (float): ì¦‰ì‹œ ë³´ìƒ
            s_next (str): ë‹¤ìŒ ìƒíƒœ
            alpha (float): í•™ìŠµë¥  Î±
            gamma (float): í• ì¸ìœ¨ Î³
        """
        best_next = max(self.Q[(s_next, a_next)] for a_next in self.actions)
        self.Q[(s,a)] = (1 - alpha)*self.Q[(s,a)] + alpha*(r + gamma*best_next)

        return None

def build_base_model() -> nn.Module:
    """
    configì— ì •ì˜ëœ í•˜ì´í¼íŒŒë¼ë¯¸í„°ë¡œ MTSTAutoencoderë¥¼ ìƒì„±í•´ ë°˜í™˜í•œë‹¤.

    Returns:
        nn.Module: ì´ˆê¸°í™”ëœ MTSTAutoencoder (ì•„ì§ state_dict ë¡œë“œ ì „)
    """
    model = MTSTAutoencoder(
        input_dim=INPUT_DIM, d_model=D_MODEL, n_heads=N_HEADS,
        seq_len=SEQ_LEN, resolutions=RESOLUTIONS, n_layers=N_LAYERS
    ).to(device)
    return model

def get_epsilon(
    global_step: int,
    total_steps: int,
    *,
    eps_decay: str = "exp",   # "exp" | "linear" | "constant"
    epsilon: float = 0.2,     # constantì¼ ë•Œ ì‚¬ìš©
    eps_start: float = 0.30,  # exp/linear ì‹œì‘ê°’
    eps_end: float = 0.05,    # ë°”ë‹¥ê°’
    eps_k: float = 3.0        # exp ê°ì‡  ê°•ë„ (í´ìˆ˜ë¡ ë¹¨ë¦¬ ì¤„ì–´ë“¦)
) -> float:
    """
    0-based global_stepê³¼ ì´ ìŠ¤í… ìˆ˜(total_steps)ë¥¼ ë°›ì•„ Îµë¥¼ ë°˜í™˜í•œë‹¤.
    """
    T = max(int(total_steps), 1)
    gs = max(0, min(int(global_step), T - 1))  # [0, T-1]ë¡œ í´ë¨í”„

    dec = eps_decay.lower()
    if dec == "constant":
        return float(epsilon)

    if dec == "exp":
        r = gs / (T - 1 if T > 1 else 1)
        e = eps_end + (eps_start - eps_end) * np.exp(-eps_k * r)
    elif dec == "linear":
        e = eps_end + (eps_start - eps_end) * (1.0 - gs / (T - 1 if T > 1 else 1))
    else:
        raise ValueError("eps_decay must be one of {'exp','linear','constant'}")

    lo, hi = (min(eps_start, eps_end), max(eps_start, eps_end))

    return float(min(max(e, lo), hi))

def run_q_learning(
    val_set,
    tr_loss_stat_path,
    q_table_path,
    states: List[str],
    actions: List[str],
    val_batch_size: int = 32,
    episodes: int = 3,
    budget_steps: Optional[int] = None,
    # ----- Îµ-greedy ê°ì‡  ì„¤ì • -----
    epsilon: float = 0.2,         # eps_decay="constant"ì¼ ë•Œ ì‚¬ìš©ë˜ëŠ” ìƒìˆ˜ Îµ
    eps_start: float = 0.30,      # ê°ì‡  ì‹œì‘ê°’ (exp/linearì—ì„œ ì‚¬ìš©)
    eps_end: float = 0.05,        # ë°”ë‹¥ê°’ (exp/linearì—ì„œ ì‚¬ìš©)
    eps_decay: str = "exp",       # "exp" | "linear" | "constant"
    eps_k: float = 3.0,           # exp ê°ì‡  ê°•ë„(í´ìˆ˜ë¡ ë¹¨ë¦¬ ì¤„ì–´ë“¦)
    # ------------------------------
    alpha_q: float = 0.2,
    gamma_q: float = 0.9,
    lr_ft: float = 5e-4,
    micro_steps: int = 1,
    allow_neg_reward: bool = True,
    seed: Optional[int] = 42,
) -> Tuple[dict, QTable]:

    set_seed(seed)

    # 1) base model load (ê¸°ì¤€ ëª¨ë¸ 1ë²ˆë§Œ ìƒì„±+ë¡œë“œ)
    base_model = build_base_model()
    base_state = torch.load(CHK_PATH['TAD'], map_location=device)
    base_model.load_state_dict(base_state)  # â† ê¸°ì¤€ ê°€ì¤‘ì¹˜
    base_model.to(device)

    # 2) Î¼, Ïƒ (pickle direct load)
    mu, sigma = load_tr_loss_stat(path=tr_loss_stat_path)

    # 3) validation batches
    idx_all = np.arange(len(val_set.tensors[0]))
    np.random.shuffle(idx_all)
    val_batches = [idx_all[i:i+val_batch_size] for i in range(0, len(idx_all), val_batch_size)]

    # ì—í”¼ì†Œë“œ ìŠ¤í… ìƒí•œ
    if budget_steps is None:
        budget_steps = len(val_batches)
    budget_steps = min(budget_steps, len(val_batches))

    # Îµ ìŠ¤ì¼€ì¤„ ì´ ìŠ¤í… ìˆ˜
    total_steps = max(episodes * budget_steps, 1)

    # 4) Q-learning
    Q = QTable(states=states, actions=actions)
    best_total_reward = -1e18
    best_state_dict   = copy.deepcopy(base_model.state_dict())

    for ep in range(episodes):
        print(f"\n[Episode {ep+1}/{episodes}]")

        # ê¸°ì¤€ ëª¨ë¸ì˜ ì‘ì—… ì‚¬ë³¸ìœ¼ë¡œ ì—í”¼ì†Œë“œ ì‹œì‘
        model = copy.deepcopy(base_model).to(device)

        total_reward = 0.0
        order = np.random.permutation(len(val_batches))[:budget_steps]

        for step, bidx in enumerate(order, 1):
            batch_index = val_batches[bidx]
            xb = val_set.tensors[0][batch_index].to(device)

            # ê¸€ë¡œë²Œ ìŠ¤í…(0-base) & í˜„ì¬ Îµ (ì™¸ë¶€ get_epsilon ì‚¬ìš©)
            gstep = ep * budget_steps + (step - 1)
            eps_now = get_epsilon(
                gstep, total_steps,
                eps_decay=eps_decay, epsilon=epsilon,
                eps_start=eps_start, eps_end=eps_end, eps_k=eps_k
            )

            # ìƒíƒœ: FT ì „ ë°°ì¹˜ ì†ì‹¤ â†’ êµ¬ê°„í™”
            loss_before = batch_loss_eval(model, xb, BASE_DIM)
            s = bin_state(loss_before, mu, sigma)

            # Îµ-greedy í–‰ë™ ì„ íƒ
            a = random.choice(actions) if (random.random() < eps_now) else Q.greedy_action(s)

            # í–‰ë™ ì‹¤í–‰ (FT_NONE ì œì™¸ fine tuning)
            if a not in ('FT_NONE'):
                set_ft_by_action(model, a)
                opt = make_optimizer(model, lr=lr_ft)
                if opt is not None:
                    for _ in range(micro_steps):
                        model.train()
                        rec  = model(xb)
                        loss = ((rec[:, :, :BASE_DIM] - xb[:, :, :BASE_DIM])**2).mean()
                        opt.zero_grad(); loss.backward(); opt.step()

            # ë³´ìƒ: Î”batch loss
            loss_after = batch_loss_eval(model, xb, BASE_DIM)
            r = (loss_before - loss_after) / sigma
            if not allow_neg_reward:
                r = max(r, 0.0)

            # Q ì—…ë°ì´íŠ¸
            s_next = bin_state(loss_after, mu, sigma)
            Q.update(s, a, r, s_next, alpha=alpha_q, gamma=gamma_q)

            total_reward += r
            if step % 10 == 0 or step == budget_steps:
                print(f"  step {step:03d} | eps={eps_now:.3f} | state={s:>4} â†’ act={a:<7} | Î”loss={r:+.6e} | next={s_next:>4}")

        print(f"Episode reward sum = {total_reward:+.6e}")
        if total_reward > best_total_reward:
            best_total_reward = total_reward
            best_state_dict   = copy.deepcopy(model.state_dict())
    # end for

    # save Q-table
    with open(q_table_path, "wb") as f:
        pickle.dump(Q, f)
    print(f"ğŸ“ Q table saved to {q_table_path}")

    return best_state_dict, Q
