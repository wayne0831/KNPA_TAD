import pandas as pd
import numpy as np
from datetime import datetime, timedelta
import os

# ============ Í≤ΩÎ°ú ÏÑ§Ï†ï ============
att_normal = r"./data/siheung_normal.att"
att_event  = r"./data/siheung_anomaly.att"
map_path   = r"./data/siheung_map_info.xlsx"
save_dir   = r"./data/"

start_time = datetime(2025, 7, 1, 0, 0, 0)

header = [
    "SIMRUN", "TIMEINT", "DATACOLLECTIONMEASUREMENT",
    "ACCELERATION(ALL)", "DIST(ALL)", "LENGTH(ALL)", "VEHS(ALL)", "PERS(ALL)",
    "QUEUEDELAY(ALL)", "SPEEDAVGARITH(ALL)", "SPEEDAVGHARM(ALL)", "OCCUPRATE(ALL)"
]
agg_cols = ['VEHS(ALL)', 'SPEEDAVGARITH(ALL)', 'OCCUPRATE(ALL)']

def get_time_interval(hour):
    if 0 <= hour <= 7: return 0
    elif 8 <= hour <= 9: return 1
    elif 10 <= hour <= 17: return 2
    elif 18 <= hour <= 19: return 3
    elif 20 <= hour <= 23: return 4
    return -1

def convert_att_to_df(att_path):
    with open(att_path, 'r', encoding='utf-8') as infile:
        lines = infile.readlines()
    for i, line in enumerate(lines):
        if line.strip().startswith('$DATACOLLECTIONMEASUREMENTEVALUATION'):
            data_start = i + 1
            break
    else:
        raise ValueError("Îç∞Ïù¥ÌÑ∞ ÏãúÏûë ÏßÄÏ†êÏùÑ Ï∞æÏùÑ Ïàò ÏóÜÏäµÎãàÎã§.")
    data_lines = lines[data_start:]
    data_rows = [line.strip().split(";") for line in data_lines if line.strip() and not line.startswith("*")]
    df = pd.DataFrame(data_rows, columns=header)
    return df

def preprocess(df, mapping_df):
    # agg_cols Ï§ëÏóêÏÑú ÌçºÏÑºÌä∏Í∞Ä ÏÑûÏó¨ ÏûàÎäî Ïª¨Îüº Î¶¨Ïä§Ìä∏ (ÌïÑÏöîÏóê Îî∞Îùº Ï∂îÍ∞Ä)
    percent_cols = ['OCCUPRATE(ALL)']

    for col in agg_cols:
        # 1) Î¨∏ÏûêÏó¥ ÌÉÄÏûÖÏúºÎ°ú ÎßåÎì† Îí§, '%'ÏôÄ Í≥µÎ∞± Ï†úÍ±∞
        df[col] = df[col].astype(str).str.replace('%', '', regex=False).str.strip()
        # 2) Ïà´ÏûêÎ°ú Î≥ÄÌôò (coerce ‚Üí NaN), Îπà Î¨∏ÏûêÏó¥ÏùÄ NaN Ï≤òÎ¶¨
        df[col] = pd.to_numeric(df[col].replace('', np.nan), errors='coerce')
        # 3) ÌçºÏÑºÌä∏ Ïª¨ÎüºÏù¥Î©¥ 100ÏúºÎ°ú ÎÇòÎà†ÏÑú ÎπÑÏú®Î°ú Î≥ÄÌôò
        if col in percent_cols:
            df[col] = df[col] / 100

    # ÎÇòÎ®∏ÏßÄ Í≤∞Ï∏°ÏπòÎäî 0ÏúºÎ°ú Ï±ÑÏö∞Í∏∞
    df[agg_cols] = df[agg_cols].fillna(0)

    # Ïù¥Ìïò Í∏∞Ï°¥ Î°úÏßÅ Í∑∏ÎåÄÎ°ú‚Ä¶
    df['TIMEINT'] = df['TIMEINT'].astype(str)
    df['start_sec'] = df['TIMEINT'].str.extract(r'(\d+)-').astype(int)
    df['date'] = df['start_sec'].apply(lambda x: start_time + timedelta(seconds=x))
    df['DAY'] = df['date'].dt.day
    df['TimeInterval'] = df['date'].dt.hour.apply(get_time_interval)

    df['DATACOLLECTIONMEASUREMENT'] = pd.to_numeric(
        df['DATACOLLECTIONMEASUREMENT'], errors='coerce'
    ).astype('Int64')
    df = df.merge(mapping_df, how='left',
                  left_on='DATACOLLECTIONMEASUREMENT',
                  right_on='ÌòÑÏû¨ Î†àÏù∏ ID')
    df = df[~df['DATACOLLECTIONMEASUREMENT'].isin(
        mapping_df[mapping_df['Î≥ÄÌôò ÎßÅÌÅ¨ ID'].isna()]['ÌòÑÏû¨ Î†àÏù∏ ID']
    )]

    df.rename(columns={'Î≥ÄÌôò ÎßÅÌÅ¨ ID': 'LINK_ID', 'Î†àÏù∏Î≤àÌò∏': 'lane'}, inplace=True)
    return df.dropna(subset=['LINK_ID', 'lane'])


def apply_event_labels(df, start_time):
    df = df.copy()
    df['pred'] = 0
    df['elapsed_sec'] = (df['date'] - start_time).dt.total_seconds().astype(int)

    # Î∞òÏúºÎ°ú Ï§ÑÏùº Ïª¨Îüº, Îëê Î∞∞Î°ú ÎäòÎ¶¥ Ïª¨Îüº
    half_cols   = ["VEHS(ALL)", "SPEEDAVGARITH(ALL)"]
    double_cols = ["OCCUPRATE(ALL)"]

    # ÏïàÏ†ÑÌïú Ïà´ÏûêÌòï Í∞ïÏ†ú Î≥ÄÌôò
    for col in set(half_cols + double_cols):
        if col in df.columns:
            df[col] = pd.to_numeric(df[col], errors='coerce')

    event_rules = [
        {'day': 1, 'start': 33300,  'end': 35100,  'link_id': 51,  'lane': [4,5], 'pred': 1},
        {'day': 2, 'start': 119700, 'end': 120600, 'link_id': 164, 'lane': 2,     'pred': 1},
        {'day': 3, 'start': 220500, 'end': 221400, 'link_id': 113, 'lane': 2,     'pred': 1},
        {'day': 4, 'start': 321300, 'end': 323100, 'link_id': 136, 'lane': [3,4], 'pred': 1},
        {'day': 5, 'start': 346500, 'end': 432900, 'link_id': 'ALL','lane': 'ALL','pred': 1},
        {'day': 6, 'start': 461700, 'end': 468900, 'link_id': 42,  'lane': 1,     'pred': 1},
        {'day': 7, 'start': 580500, 'end': 581400, 'link_id': 67,  'lane': 2,     'pred': 1},
    ]

    for rule in event_rules:
        cond = (
            (df['DAY'] == rule['day']) &
            df['elapsed_sec'].between(rule['start'], rule['end'])
        )
        if rule['link_id'] != 'ALL':
            cond &= (df['LINK_ID'] == rule['link_id'])
        if rule['lane'] != 'ALL':
            if isinstance(rule['lane'], (list, tuple, set)):
                cond &= df['lane'].isin(rule['lane'])
            else:
                cond &= (df['lane'] == rule['lane'])

        # pred Ìï†Îãπ
        df.loc[cond, 'pred'] = rule['pred']

        # Î∞òÏúºÎ°ú Ï§ÑÏù¥Í∏∞
        for col in half_cols:
            if col in df.columns and np.issubdtype(df[col].dtype, np.number):
                df.loc[cond, col] = df.loc[cond, col] * 0.5

        # Îëê Î∞∞Î°ú ÎäòÎ¶¨Í∏∞
        for col in double_cols:
            if col in df.columns and np.issubdtype(df[col].dtype, np.number):
                df.loc[cond, col] = df.loc[cond, col] * 2

    return df

def apply_bigo_mapping(df):
    has_note = df['ÎπÑÍ≥†'].notna()
    df_with_note = df[has_note].copy()
    df_with_note[agg_cols] = df_with_note[agg_cols].fillna(0)
    df_with_note['LINK_ID'] = df_with_note['ÎπÑÍ≥†']

    group_cols = ['date', 'lane', 'LINK_ID']
    df_grouped_mean = df_with_note.groupby(group_cols, as_index=False)[agg_cols].mean()
    df_grouped_mean['VEHS(ALL)'] = df_grouped_mean['VEHS(ALL)'].round().astype(int)
    meta_cols = ['DAY', 'TimeInterval', 'pred']
    df_grouped_meta = df_with_note.groupby(group_cols, as_index=False).first()[group_cols + meta_cols]
    df_note_final = pd.merge(df_grouped_mean, df_grouped_meta, on=group_cols)

    df_without_note = df[~has_note][['date', 'LINK_ID', 'DAY', 'TimeInterval', 'lane'] + agg_cols + ['pred']]
    return pd.concat([df_note_final, df_without_note], ignore_index=True)

# ‚úÖ One-hot encoding Ìï®Ïàò
def one_hot_encode_time_interval(df):
    one_hot = pd.get_dummies(df['TimeInterval'], prefix='TimeInt')
    df = pd.concat([df.drop(columns=['TimeInterval']), one_hot], axis=1)
    return df

# ======================== Ïã§Ìñâ ========================
print(".att ÌååÏùº Î°úÎî©")
df_normal = convert_att_to_df(att_normal)
df_event  = convert_att_to_df(att_event)


mapping_df = pd.read_excel(map_path, sheet_name="Î≥ÄÌôò")
adj_df = pd.read_excel(map_path, sheet_name="Ïù∏Ï†ë")

print("Ï†ÑÏ≤òÎ¶¨ ÏàòÌñâ Ï§ë")
df_normal_cleaned = preprocess(df_normal, mapping_df)
df_event_cleaned = preprocess(df_event, mapping_df)

print(df_normal_cleaned.head())

"""
print("pred ÎùºÎ≤®ÎßÅ Ï†ÅÏö© Ï§ë")
df_normal_cleaned['pred'] = 0
df_event_labeled = apply_event_labels(df_event_cleaned, start_time)

print("ÎßÅÌÅ¨ Í∑∏Î£π ÌÜµÌï© Ï§ë")
df_normal_final = apply_bigo_mapping(df_normal_cleaned)
df_event_final = apply_bigo_mapping(df_event_labeled)

# ‚úÖ One-hot encoding Ï†ÅÏö©
print("One-hot encoding Ï†ÅÏö© Ï§ë (TimeInterval)")
df_train = df_normal_final[df_normal_final['DAY'] <= 5]
df_val   = df_normal_final[df_normal_final['DAY'] > 5]
df_test  = df_event_final

df_train = one_hot_encode_time_interval(df_train)
df_val   = one_hot_encode_time_interval(df_val)
df_test  = one_hot_encode_time_interval(df_test)

from sklearn.preprocessing import StandardScaler  # ‚Üê Ï∂îÍ∞Ä

scaler = StandardScaler()
# 1) train ÏúºÎ°ú fit + transform
df_train[agg_cols] = scaler.fit_transform(df_train[agg_cols])

# 2) val/test Îäî transform Îßå
df_val[agg_cols]   = scaler.transform(df_val[agg_cols])
df_test[agg_cols]  = scaler.transform(df_test[agg_cols])

# Ï†ÄÏû•
train_path = os.path.join(save_dir, "converted_train_0812.csv")
val_path = os.path.join(save_dir, "converted_val_0812.csv")
test_path = os.path.join(save_dir, "converted_test_0812.csv")

df_train.to_csv(train_path, index=False)
df_val.to_csv(val_path, index=False)
df_test.to_csv(test_path, index=False)

print(f"\n‚úÖ Ï†ÑÏ≤¥ Ï≤òÎ¶¨ ÏôÑÎ£å!\nüìÅ Ï†ÄÏû• Í≤ΩÎ°ú:\n- {train_path}\n- {val_path}\n- {test_path}")
"""