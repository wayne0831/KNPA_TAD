
###########################################################################################################
# import libraries
###########################################################################################################

# -*- coding: utf-8 -*-
"""
Created on Wed Sep 24 09:24:42 2025

@author: user
"""

from config import *
import pandas as pd
import numpy as np
import re
from datetime import timedelta, datetime
import os


###########################################################################################################
# set user-defined functions
###########################################################################################################

def preprocess(data_path, infer=False, seq_len=30, tr_ratio=0.7, val_ratio=0.2, te_ratio=0.1, event_rules=None, start_time=None):
    """
    ë°ì´í„° ì „ì²˜ë¦¬ í•¨ìˆ˜.
    """
    
    raw_path = DATA_PATH[data_path]['raw']
    tr_path = DATA_PATH[data_path]['tr']
    val_path = DATA_PATH[data_path]['val']
    te_path = DATA_PATH[data_path]['te']
    infer_path = DATA_PATH[data_path]['infer']

    print(f"ë°ì´í„° ë¡œë“œ: {raw_path}")
    df = pd.read_csv(raw_path, encoding='cp949')
    
    # ğŸš¨ ìˆ˜ì •: ì—´ ì´ë¦„ì˜ ì•ë’¤ ê³µë°±ì„ ì œê±°í•˜ê³ , 'ACRS_ID' ëŒ€ì‹  'ACSR_ID'ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.
    df.columns = df.columns.str.strip() 

    # TODO: NODE_ID + ACSR_ID -> LINK_ID ìƒì„±(ìˆ˜ì • ì™„ë£Œ)
    df['LINK_ID'] = df['NODE_ID'].astype(str) + '_' + df['ACSR_ID'].astype(str)
    
    # -----------------------------------------------------------
    # 1. TOT_DT Datetime ë³€í™˜ ë° êµì²´
    # -----------------------------------------------------------
    time_parts = df['TOT_DT'].str.extract(r'(\d+-\d+-\d+)\s+(ì˜¤ì „|ì˜¤í›„)\s+(\d+):(\d+)')
    time_parts.columns = ['date_str', 'ampm', 'hour', 'minute']
    
    time_parts['hour'] = pd.to_numeric(time_parts['hour'])
    time_parts['minute'] = pd.to_numeric(time_parts['minute'])
    
    # 24ì‹œê°„ í˜•ì‹ ì‹œ(Hour) ê³„ì‚°
    time_parts.loc[
        (time_parts['ampm'] == 'ì˜¤í›„') & (time_parts['hour'] < 12),
        'hour'
    ] += 12
    
    time_parts.loc[
        (time_parts['ampm'] == 'ì˜¤ì „') & (time_parts['hour'] == 12),
        'hour'
    ] = 0
    
    # í‘œì¤€ Datetime ë¬¸ìì—´ ì¡°í•©
    def expand_year(date_str):
        parts = date_str.split('-')
        return f"20{parts[0]}-{parts[1].zfill(2)}-{parts[2].zfill(2)}"
    
    time_parts['DATE_STANDARD'] = time_parts['date_str'].apply(expand_year)
    time_parts['TIME_STANDARD'] = (
        time_parts['hour'].astype(str).str.zfill(2) + ':' +
        time_parts['minute'].astype(str).str.zfill(2) + ':00'
    )
    
    df['TOT_DT_DATETIME'] = pd.to_datetime(
        time_parts['DATE_STANDARD'] + ' ' + time_parts['TIME_STANDARD'],
        format='%Y-%m-%d %H:%M:%S',
        errors='coerce'
    )

    # ê¸°ì¡´ TOT_DT ì‚­ì œ ë° Datetime ì—´ë¡œ êµì²´
    df = df.drop(columns=['TOT_DT'])
    df = df.rename(columns={'TOT_DT_DATETIME': 'TOT_DT'})
    
    # -----------------------------------------------------------
    # 2. ì‹œê°„ ì •ê·œí™” ë° ëˆ„ë½ ì‹œê°„ ì±„ìš°ê¸°
    # -----------------------------------------------------------

    # 2.1. ì‹œê°„ ë²”ìœ„ ê²°ì • ë° 1ë¶„ ê°„ê²© ê¸°ì¤€ ì‹œê°„í‘œ ìƒì„±
    min_dt = df['TOT_DT'].min()
    max_dt = df['TOT_DT'].max()

    time_index = pd.date_range(start=min_dt, end=max_dt, freq='1T')
    group_keys_base = df[['LINK_ID', 'LANE_NO']].drop_duplicates().reset_index(drop=True)
    
    master_index = pd.MultiIndex.from_product(
        [time_index, group_keys_base['LINK_ID'], group_keys_base['LANE_NO']],
        names=['TOT_DT', 'LINK_ID', 'LANE_NO']
    ).to_frame(index=False)
    
    # 2.2. ê¸°ì¡´ ë°ì´í„° ì§‘ê³„ (ì¤‘ë³µ ì‹œê°„ ì²˜ë¦¬)
    agg_funcs = {
        'TRF_QNTY': 'sum',
        'AVG_SPD': 'mean',
        'OCPN_RATE': 'mean'
    }

    aggregated_df = df.groupby(GRP_COLS)[INPUT_COLS].agg(agg_funcs).reset_index()

    # 2.3. ê¸°ì¤€ í…Œì´ë¸”ê³¼ ì§‘ê³„ ë°ì´í„° ë³‘í•© (Outer Merge)
    final_df = pd.merge(
        master_index, 
        aggregated_df, 
        on=GRP_COLS, 
        how='left'
    )

    # 2.4. ëˆ„ë½ëœ êµí†µëŸ‰ ë°ì´í„° (NaN)ë¥¼ 0ìœ¼ë¡œ ì±„ìš°ê¸°
    final_df[INPUT_COLS] = final_df[INPUT_COLS].fillna(0)
    
    # ìµœì¢… ë°ì´í„°í”„ë ˆì„ì„ dfì— í• ë‹¹
    df = final_df

    # -----------------------------------------------------------
    # 3. TimeInterval, pred ìƒì„± ë° ë”ë¯¸ ë³€ìˆ˜í™”
    # -----------------------------------------------------------
    
    # 'pred' ì—´ ìƒì„±
    df['pred'] = 0 
    
    # TimeInterval í•¨ìˆ˜ ì •ì˜
    def get_time_interval(hour):
        if    0 <= hour <= 7:  return 0
        elif  8 <= hour <= 9:  return 1
        elif 10 <= hour <= 17: return 2
        elif 18 <= hour <= 19: return 3
        elif 20 <= hour <= 23: return 4
        return -1
    
    # TimeInterval ì—´ ìƒì„±
    df['TimeInterval'] = df['TOT_DT'].dt.hour.apply(get_time_interval)
    
    all_intervals = [0, 1, 2, 3, 4]
    
    # ë²”ì£¼í˜• ë³€í™˜ ë° ë”ë¯¸ ë³€ìˆ˜ ìƒì„±
    df['TimeInterval'] = pd.Categorical(df['TimeInterval'], categories=all_intervals)
    df = pd.get_dummies(df, columns=['TimeInterval'], prefix='TimeInt')

    # -----------------------------------------------------------
    # 4. ìµœì¢… ì»¬ëŸ¼ ì •ë¦¬
    # -----------------------------------------------------------
    
    final_timeint_cols = [col for col in df.columns if 'TimeInt' in col]
    final_cols = GRP_COLS + INPUT_COLS + final_timeint_cols + ['pred']
    
    # ìµœì¢… ì»¬ëŸ¼ë§Œ ë‚¨ê¸°ê³  ìˆœì„œ ì¬ë°°ì—´, ëˆ„ë½ëœ ì»¬ëŸ¼ 0 ì±„ìš°ê¸°, ì¤‘ë³µ í–‰ ì œê±°
    df = df.reindex(columns=final_cols, fill_value=0).drop_duplicates()
    
    # -----------------------------------------------------------
    # 5. ë°ì´í„° ë¶„í•  ë° ì €ì¥ (ìš”ì²­í•˜ì‹  ìµœì¢… ë¡œì§)
    # -----------------------------------------------------------
    
    # ì •ë ¬ ê¸°ì¤€ ì»¬ëŸ¼ ì„¤ì • (TOT_DTê°€ Datetime íƒ€ì…ìœ¼ë¡œ ì •ë ¬ ê¸°ì¤€ì´ ë¨)
    group_by_cols = [col for col in GRP_COLS if col != 'TOT_DT'] # ['LINK_ID', 'LANE_NO']

    if infer:
        print("ì¶”ë¡ ìš© ë°ì´í„° ì¶”ì¶œ ëª¨ë“œ...")
        infer_df_list = []
        
        # LINK_ID, LANE_NO ë³„ë¡œ ê·¸ë£¹í™”
        groups = df.groupby(group_by_cols) 
        
        for _, group in groups:
            # TOT_DT ê¸°ì¤€ìœ¼ë¡œ ì •ë ¬ (ì´ë¯¸ ì •ê·œí™” ë‹¨ê³„ì—ì„œ ì •ë ¬ë˜ì–´ ìˆì„ ê°€ëŠ¥ì„±ì´ ë†’ì§€ë§Œ ì•ˆì „í•˜ê²Œ ì¬ì •ë ¬)
            group_sorted = group.sort_values(by='TOT_DT') 
            latest_data = group_sorted.tail(seq_len)
            infer_df_list.append(latest_data)
            
        infer_df = pd.concat(infer_df_list)
        infer_df.to_csv(infer_path, index=False)
        print(f"ì¶”ë¡ ìš© ë°ì´í„° ì €ì¥ ì™„ë£Œ: {infer_path}")


    else:
        print("ë°ì´í„° ë¶„í•  ëª¨ë“œ...")
        
        # ì‹œê³„ì—´ ìˆœì„œëŒ€ë¡œ ìˆœì°¨ ë¶„í• 
        total_rows = len(df)
        tr_end = int(total_rows * tr_ratio)
        val_end = tr_end + int(total_rows * val_ratio)
        
        tr_df = df.iloc[:tr_end]
        val_df = df.iloc[tr_end:val_end]
        te_df = df.iloc[val_end:]
        
        tr_df.to_csv(tr_path, index=False)
        val_df.to_csv(val_path, index=False)
        te_df.to_csv(te_path, index=False)
        
        print(f"í•™ìŠµ ë°ì´í„° ì €ì¥ ì™„ë£Œ: {tr_path}")
        print(f"ê²€ì¦ ë°ì´í„° ì €ì¥ ì™„ë£Œ: {val_path}")
        print(f"í…ŒìŠ¤íŠ¸ ë°ì´í„° ì €ì¥ ì™„ë£Œ: {te_path}")


# raw dataë¶ˆëŸ¬ì™€ì„œ train/valid/testë¡œ êµ¬ë¶„
# TODO: ì—¬ê¸°ì„œëŠ” scaling ì „ ë°ì´í„°ê°€ í´ë”ì— ì €ì¥ë˜ì–´ì•¼ í•¨
preprocess(data_path   = TAD_VER, 
           infer       = False, 
           seq_len     = SEQ_LEN, 
           tr_ratio    = 0.7, 
           val_ratio   = 0.2, 
           te_ratio    = 0.1, # ì‹¤ì œ ìš´ì˜ì‹œì—ëŠ” testset í•„ìš”ì—†ìŒ
           event_rules = None, 
           start_time  = None)


