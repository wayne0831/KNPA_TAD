###########################################################################################################
# import libraries
###########################################################################################################

import os
from config import *
from TAD_model import *
from TAD_result_analysis import *
from sklearn.metrics import precision_score, recall_score, f1_score, classification_report, roc_auc_score, average_precision_score
from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay
from RL_model import *  # run_q_learning, QTable ë“±

###########################################################################################################
# load/preprcoess data
###########################################################################################################

############### TODO ë°ì´í„° load, ê°€ê³µ(scaling í¬í•¨), ì €ì¥: HJL ì§€ì› í•„ìš”
# ë°œê²¬ ì—ëŸ¬
# 1. converted_test 7ì›”1ì¼ë¶€í„° ì‹œì‘
## start_time ê´€ë ¨ ì‘ì—… í•„ìš”
### ìµœì¢…: scalingëœ ë°ì´í„°ì…‹ ì €ì¥

###########################################################################################################
# utils
###########################################################################################################

def print_q_table(q_table):
    """(ì˜µì…˜) í•™ìŠµëœ Q í…Œì´ë¸” ê°„ë‹¨ ì¶œë ¥"""
    print("\n=== Learned Q(s,a) ===")
    states, actions = q_table.states, q_table.actions
    header = "state \\ action".ljust(12) + " | " + " | ".join(a.ljust(8) for a in actions)
    print(header); print("-" * len(header))
    for s in states:
        row = s.ljust(12) + " | " + " | ".join(f"{q_table.Q[(s,a)]:+8.3e}" for a in actions)
        print(row)
    print("=" * len(header))


###########################################################################################################
# run modeling - model.py, result_analysis.py
###########################################################################################################

def main():
    # 1) ê²€ì¦ ë°ì´í„° ë¡œë“œ (Î¼,ÏƒëŠ” pickleì—ì„œ ì§ì ‘ ë¡œë“œí•˜ë¯€ë¡œ train_set ë¶ˆí•„ìš”)
    print('============== Data load for RL ==============')
    val_set, val_meta = load_dataset(csv_path=DATA_PATH[TARGET_DATA]['val'], seq_len=SEQ_LEN, stride=STRIDE)

    # 2) Q-learning ì‹¤í–‰ (Îµ ê°ì‡  ìŠ¤ì¼€ì¤„ ì¸ì ì „ë‹¬)
    best_state_dict, q_table = run_q_learning(
        val_set        = val_set,
        tr_loss_stat_path= PICKLE_PATH['TAD']['tr_loss_stat'],
        q_table_path   = PICKLE_PATH['RL']['q_table'],
        states         = STATE,
        actions        = ACTIONS,
        val_batch_size = BATCH_SIZE,
        episodes       = EPISODES,
        budget_steps   = BUDGET_STEPS,
        # --- Îµ-greedy ê°ì‡  ì„¤ì • (config ì—†ìœ¼ë©´ ê¸°ë³¸ê°’ìœ¼ë¡œ) ---
        epsilon        = EPSILON,       # eps_decay="constant"ì¼ ë•Œ ì‚¬ìš©
        eps_start      = EPS_START,
        eps_end        = EPS_END,
        eps_decay      = EPS_DECAY,     # 'exp' | 'linear' | 'constant'
        eps_k          = EPS_K,
        # ---------------------------------------------------
        alpha_q        = ALPHA_Q,
        gamma_q        = GAMMA_Q,
        lr_ft          = LR_FT,
        micro_steps    = MICRO_STEPS,
        allow_neg_reward = ALLOW_NEG_REWARD,
        seed           = SEED
    )

    # (ì˜µì…˜) Q-í…Œì´ë¸” ì¶œë ¥
    print_q_table(q_table)

    # 3) RL ì²´í¬í¬ì¸íŠ¸ ì €ì¥
    os.makedirs(os.path.dirname(CHK_PATH['RL']), exist_ok=True)
    torch.save(best_state_dict, CHK_PATH['RL'])
    print(f"\nğŸ“ RL-finetuned model saved to {CHK_PATH['RL']}")

if __name__ == "__main__":
    main()
