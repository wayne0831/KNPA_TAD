###########################################################################################################
# import libraries
###########################################################################################################

from config import *
from data_preprocess import *
import pandas as pd
from datetime import datetime, timedelta

###########################################################################################################
# load data
###########################################################################################################

#
df_raw = pd.read_csv(DATA_PATH)
df     = df_raw

df_melted = df.melt(id_vars=["TOT_DT", "LINK_ID", "LANE_NO"], var_name="Metric", value_name="Value")

df_melted["Metric"] = df_melted["Metric"] + "_LINK" + df_melted["LINK_ID"].astype(str) + "_LANE" + df_melted["LANE_NO"].astype(str)

df_pivoted = df_melted.pivot(index="TOT_DT", columns="Metric", values="Value").reset_index()

#print(sorted(set(df_pivoted['TOT_DT'])))
#print(df_pivoted)

#
# df_pivoted.to_csv(TR_PATH, index=False)
# df_pivoted.to_csv(VAL_PATH, index=False)
# df_pivoted.to_csv(TR_PATH, index=False)

###########################################################################################################
# load data
###########################################################################################################

import pandas as pd
import numpy as np
from datetime import datetime, timedelta
import os

# ============ ê²½ë¡œ ì„¤ì • ============
att_normal = r"C:\Users\user\Downloads\ì‹œí¥ ë°°ê³§_ëŒë°œ ì—†ëŠ” ë²„ì „ - Scenario 1_Data Collection Results_001.att"
att_event = r"C:\Users\user\Downloads\ì‹œí¥ ë°°ê³§_ëŒë°œ_Data Collection Results_001.att"
map_path = r"C:\Users\user\Downloads\ì‹œí¥ ë°°ê³§ ì •ë°€ë„ë¡œì§€ë„-VISSIM LINK ID ë§µí•‘_1.xlsx"
save_dir = r"C:\Users\user\Downloads"

start_time = datetime(2025, 7, 1, 0, 0, 0)

header = [
    "SIMRUN", "TIMEINT", "DATACOLLECTIONMEASUREMENT",
    "ACCELERATION(ALL)", "DIST(ALL)", "LENGTH(ALL)", "VEHS(ALL)", "PERS(ALL)",
    "QUEUEDELAY(ALL)", "SPEEDAVGARITH(ALL)", "SPEEDAVGHARM(ALL)", "OCCUPRATE(ALL)"
]
agg_cols = ['VEHS(ALL)', 'SPEEDAVGARITH(ALL)', 'OCCUPRATE(ALL)']

def get_time_interval(hour):
    if 0 <= hour <= 7: return 0
    elif 8 <= hour <= 9: return 1
    elif 10 <= hour <= 17: return 2
    elif 18 <= hour <= 19: return 3
    elif 20 <= hour <= 23: return 4
    return -1

def convert_att_to_df(att_path):
    with open(att_path, 'r', encoding='utf-8') as infile:
        lines = infile.readlines()
    for i, line in enumerate(lines):
        if line.strip().startswith('$DATACOLLECTIONMEASUREMENTEVALUATION'):
            data_start = i + 1
            break
    else:
        raise ValueError("ë°ì´í„° ì‹œì‘ ì§€ì ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
    data_lines = lines[data_start:]
    data_rows = [line.strip().split(";") for line in data_lines if line.strip() and not line.startswith("*")]
    df = pd.DataFrame(data_rows, columns=header)
    return df

def preprocess(df, mapping_df):
    # agg_cols ì¤‘ì—ì„œ í¼ì„¼íŠ¸ê°€ ì„ì—¬ ìˆëŠ” ì»¬ëŸ¼ ë¦¬ìŠ¤íŠ¸ (í•„ìš”ì— ë”°ë¼ ì¶”ê°€)
    percent_cols = ['OCCUPRATE(ALL)']

    for col in agg_cols:
        # 1) ë¬¸ìì—´ íƒ€ì…ìœ¼ë¡œ ë§Œë“  ë’¤, '%'ì™€ ê³µë°± ì œê±°
        df[col] = df[col].astype(str).str.replace('%', '', regex=False).str.strip()
        # 2) ìˆ«ìë¡œ ë³€í™˜ (coerce â†’ NaN), ë¹ˆ ë¬¸ìì—´ì€ NaN ì²˜ë¦¬
        df[col] = pd.to_numeric(df[col].replace('', np.nan), errors='coerce')
        # 3) í¼ì„¼íŠ¸ ì»¬ëŸ¼ì´ë©´ 100ìœ¼ë¡œ ë‚˜ëˆ ì„œ ë¹„ìœ¨ë¡œ ë³€í™˜
        if col in percent_cols:
            df[col] = df[col] / 100

    # ë‚˜ë¨¸ì§€ ê²°ì¸¡ì¹˜ëŠ” 0ìœ¼ë¡œ ì±„ìš°ê¸°
    df[agg_cols] = df[agg_cols].fillna(0)

    # ì´í•˜ ê¸°ì¡´ ë¡œì§ ê·¸ëŒ€ë¡œâ€¦
    df['TIMEINT'] = df['TIMEINT'].astype(str)
    df['start_sec'] = df['TIMEINT'].str.extract(r'(\d+)-').astype(int)
    df['date'] = df['start_sec'].apply(lambda x: start_time + timedelta(seconds=x))
    df['DAY'] = df['date'].dt.day
    df['TimeInterval'] = df['date'].dt.hour.apply(get_time_interval)

    df['DATACOLLECTIONMEASUREMENT'] = pd.to_numeric(
        df['DATACOLLECTIONMEASUREMENT'], errors='coerce'
    ).astype('Int64')
    df = df.merge(mapping_df, how='left',
                  left_on='DATACOLLECTIONMEASUREMENT',
                  right_on='í˜„ì¬ ë ˆì¸ ID')
    df = df[~df['DATACOLLECTIONMEASUREMENT'].isin(
        mapping_df[mapping_df['ë³€í™˜ ë§í¬ ID'].isna()]['í˜„ì¬ ë ˆì¸ ID']
    )]

    df.rename(columns={'ë³€í™˜ ë§í¬ ID': 'LINK_ID', 'ë ˆì¸ë²ˆí˜¸': 'lane'}, inplace=True)
    return df.dropna(subset=['LINK_ID', 'lane'])


def apply_event_labels(df, start_time):
    df = df.copy()
    df['pred'] = 0
    df['elapsed_sec'] = (df['date'] - start_time).dt.total_seconds().astype(int)

    # ë°˜ìœ¼ë¡œ ì¤„ì¼ ì»¬ëŸ¼, ë‘ ë°°ë¡œ ëŠ˜ë¦´ ì»¬ëŸ¼
    half_cols   = ["VEHS(ALL)", "SPEEDAVGARITH(ALL)"]
    double_cols = ["OCCUPRATE(ALL)"]

    # ì•ˆì „í•œ ìˆ«ìí˜• ê°•ì œ ë³€í™˜
    for col in set(half_cols + double_cols):
        if col in df.columns:
            df[col] = pd.to_numeric(df[col], errors='coerce')

    event_rules = [
        {'day': 1, 'start': 33300,  'end': 35100,  'link_id': 51,  'lane': [4,5], 'pred': 1},
        {'day': 2, 'start': 119700, 'end': 120600, 'link_id': 164, 'lane': 2,     'pred': 1},
        {'day': 3, 'start': 220500, 'end': 221400, 'link_id': 113, 'lane': 2,     'pred': 1},
        {'day': 4, 'start': 321300, 'end': 323100, 'link_id': 136, 'lane': [3,4], 'pred': 1},
        {'day': 5, 'start': 346500, 'end': 432900, 'link_id': 'ALL','lane': 'ALL','pred': 1},
        {'day': 6, 'start': 461700, 'end': 468900, 'link_id': 42,  'lane': 1,     'pred': 1},
        {'day': 7, 'start': 580500, 'end': 581400, 'link_id': 67,  'lane': 2,     'pred': 1},
    ]

    for rule in event_rules:
        cond = (
            (df['DAY'] == rule['day']) &
            df['elapsed_sec'].between(rule['start'], rule['end'])
        )
        if rule['link_id'] != 'ALL':
            cond &= (df['LINK_ID'] == rule['link_id'])
        if rule['lane'] != 'ALL':
            if isinstance(rule['lane'], (list, tuple, set)):
                cond &= df['lane'].isin(rule['lane'])
            else:
                cond &= (df['lane'] == rule['lane'])

        # pred í• ë‹¹
        df.loc[cond, 'pred'] = rule['pred']

        # ë°˜ìœ¼ë¡œ ì¤„ì´ê¸°
        for col in half_cols:
            if col in df.columns and np.issubdtype(df[col].dtype, np.number):
                df.loc[cond, col] = df.loc[cond, col] * 0.5

        # ë‘ ë°°ë¡œ ëŠ˜ë¦¬ê¸°
        for col in double_cols:
            if col in df.columns and np.issubdtype(df[col].dtype, np.number):
                df.loc[cond, col] = df.loc[cond, col] * 2

    return df

def apply_bigo_mapping(df):
    has_note = df['ë¹„ê³ '].notna()
    df_with_note = df[has_note].copy()
    df_with_note[agg_cols] = df_with_note[agg_cols].fillna(0)
    df_with_note['LINK_ID'] = df_with_note['ë¹„ê³ ']

    group_cols = ['date', 'lane', 'LINK_ID']
    df_grouped_mean = df_with_note.groupby(group_cols, as_index=False)[agg_cols].mean()
    df_grouped_mean['VEHS(ALL)'] = df_grouped_mean['VEHS(ALL)'].round().astype(int)
    meta_cols = ['DAY', 'TimeInterval', 'pred']
    df_grouped_meta = df_with_note.groupby(group_cols, as_index=False).first()[group_cols + meta_cols]
    df_note_final = pd.merge(df_grouped_mean, df_grouped_meta, on=group_cols)

    df_without_note = df[~has_note][['date', 'LINK_ID', 'DAY', 'TimeInterval', 'lane'] + agg_cols + ['pred']]
    return pd.concat([df_note_final, df_without_note], ignore_index=True)

# âœ… One-hot encoding í•¨ìˆ˜
def one_hot_encode_time_interval(df):
    one_hot = pd.get_dummies(df['TimeInterval'], prefix='TimeInt')
    df = pd.concat([df.drop(columns=['TimeInterval']), one_hot], axis=1)
    return df

# ======================== ì‹¤í–‰ ========================
print(".att íŒŒì¼ ë¡œë”©")
df_normal = convert_att_to_df(att_normal)
df_event = convert_att_to_df(att_event)
mapping_df = pd.read_excel(map_path, sheet_name="ë³€í™˜")
adj_df = pd.read_excel(map_path, sheet_name="ì¸ì ‘")

print("ì „ì²˜ë¦¬ ìˆ˜í–‰ ì¤‘")
df_normal_cleaned = preprocess(df_normal, mapping_df)
df_event_cleaned = preprocess(df_event, mapping_df)

print("pred ë¼ë²¨ë§ ì ìš© ì¤‘")
df_normal_cleaned['pred'] = 0
df_event_labeled = apply_event_labels(df_event_cleaned, start_time)

print("ë§í¬ ê·¸ë£¹ í†µí•© ì¤‘")
df_normal_final = apply_bigo_mapping(df_normal_cleaned)
df_event_final = apply_bigo_mapping(df_event_labeled)

# âœ… One-hot encoding ì ìš©
print("One-hot encoding ì ìš© ì¤‘ (TimeInterval)")
df_train = df_normal_final[df_normal_final['DAY'] <= 5]
df_val   = df_normal_final[df_normal_final['DAY'] > 5]
df_test  = df_event_final

df_train = one_hot_encode_time_interval(df_train)
df_val   = one_hot_encode_time_interval(df_val)
df_test  = one_hot_encode_time_interval(df_test)

from sklearn.preprocessing import StandardScaler  # â† ì¶”ê°€

scaler = StandardScaler()
# 1) train ìœ¼ë¡œ fit + transform
df_train[agg_cols] = scaler.fit_transform(df_train[agg_cols])

# 2) val/test ëŠ” transform ë§Œ
df_val[agg_cols]   = scaler.transform(df_val[agg_cols])
df_test[agg_cols]  = scaler.transform(df_test[agg_cols])

# ì €ì¥
train_path = os.path.join(save_dir, "converted_train.csv")
val_path = os.path.join(save_dir, "converted_val.csv")
test_path = os.path.join(save_dir, "converted_test.csv")

df_train.to_csv(train_path, index=False)
df_val.to_csv(val_path, index=False)
df_test.to_csv(test_path, index=False)

print(f"\nâœ… ì „ì²´ ì²˜ë¦¬ ì™„ë£Œ!\nğŸ“ ì €ì¥ ê²½ë¡œ:\n- {train_path}\n- {val_path}\n- {test_path}")